{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/gwy/miniconda3/envs/torch25/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from accelerate import PartialState\n",
    "from datasets import Dataset, IterableDataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "local_ranks = 3\n",
    "\n",
    "model_path = \"/data/public/model/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16)\n",
    "model.to(local_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_from_disk\n",
    "\n",
    "train_dataset = load_from_disk(\"/data4/gwy/R1/understand-r1-zero-main/datasets/train/math_12k/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'How many units long is a segment whose endpoints are $(-4,1)$ and $(1,13)$?',\n",
       " 'solution': 'We use the distance formula: $\\\\sqrt{(-4 - 1)^2 + (1 - 13)^2},$ which is $\\\\sqrt{25 + 144} = \\\\sqrt{169} = \\\\boxed{13}$.\\n\\n- OR -\\n\\nWe note that the points $(-4,1)$, $(1,13)$, and $(1,1)$ form a right triangle with legs of length 5 and 12. $(5,12,13)$ is a Pythagorean triple, so the hypotenuse has length $\\\\boxed{13}$.',\n",
       " 'answer': '13',\n",
       " 'subject': 'Algebra',\n",
       " 'level': 2,\n",
       " 'unique_id': 'test/algebra/1570.json',\n",
       " 'gold_solution_steps': ['We use the distance formula:',\n",
       "  '$\\\\sqrt{(-4 - 1)^2 + (1 - 13)^2},$ which is $\\\\sqrt{25 + 144} = \\\\sqrt{169} = \\\\boxed{13}$.',\n",
       "  '- OR - We note that the points $(-4,1)$, $(1,13)$,',\n",
       "  'and $(1,1)$ form a right triangle with legs of length 5 and 12. $(5,12,13)$ is a Pythagorean triple, so the hypotenuse has length $\\\\boxed{13}$.']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format into conversation\n",
    "def make_r1_conversation(example):\n",
    "    return {\n",
    "        \"prompt\": \"A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. \"\n",
    "    \"The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\\nUser: \"\n",
    "    + example[\"problem\"]\n",
    "    + \"\\nAssistant: <think>\",\n",
    "        \"solution\": example['solution']\n",
    "    }\n",
    "\n",
    "train_dataset = train_dataset.map(make_r1_conversation, remove_columns=train_dataset.column_names, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solution': 'We use the distance formula: $\\\\sqrt{(-4 - 1)^2 + (1 - 13)^2},$ which is $\\\\sqrt{25 + 144} = \\\\sqrt{169} = \\\\boxed{13}$.\\n\\n- OR -\\n\\nWe note that the points $(-4,1)$, $(1,13)$, and $(1,1)$ form a right triangle with legs of length 5 and 12. $(5,12,13)$ is a Pythagorean triple, so the hypotenuse has length $\\\\boxed{13}$.',\n",
       " 'prompt': 'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\\nUser: How many units long is a segment whose endpoints are $(-4,1)$ and $(1,13)$?\\nAssistant: <think>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 16:52:45,589\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 16:52:45 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-24 16:52:50 config.py:549] This model supports multiple tasks: {'score', 'classify', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-24 16:52:50 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/data/public/model/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='/data/public/model/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:3, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/data/public/model/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-24 16:52:52 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-24 16:52:52 model_runner.py:1110] Starting to load model /data/public/model/Qwen2.5-1.5B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.06it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.05it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 16:52:53 model_runner.py:1115] Loading model weights took 2.8875 GB\n",
      "INFO 03-24 16:52:54 worker.py:267] Memory profiling takes 0.81 seconds\n",
      "INFO 03-24 16:52:54 worker.py:267] the current vLLM instance can use total_gpu_memory (79.33GiB) x gpu_memory_utilization (0.70) = 55.53GiB\n",
      "INFO 03-24 16:52:54 worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 2.02GiB; the rest of the memory reserved for KV Cache is 50.52GiB.\n",
      "INFO 03-24 16:52:54 executor_base.py:111] # cuda blocks: 118254, # CPU blocks: 9362\n",
      "INFO 03-24 16:52:54 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 57.74x\n",
      "INFO 03-24 16:52:56 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 16:53:08 model_runner.py:1562] Graph capturing finished in 12 secs, took 0.21 GiB\n",
      "INFO 03-24 16:53:08 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 15.63 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "vllm_device = \"cuda:3\"\n",
    "llm = LLM(\n",
    "    model=model_path ,\n",
    "    device=vllm_device,\n",
    "    gpu_memory_utilization=0.7,\n",
    "    dtype=\"auto\",\n",
    "    enable_prefix_caching=True,\n",
    "    max_model_len=None,\n",
    ")\n",
    "sampling_params = SamplingParams(\n",
    "temperature=0.9,\n",
    "max_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 228]) torch.Size([16, 228])\n"
     ]
    }
   ],
   "source": [
    "from trl.data_utils import apply_chat_template, is_conversational, maybe_apply_chat_template\n",
    "\n",
    "device =  \"cuda:0\"\n",
    "inputs = train_dataset.select(range(16))\n",
    "prompts = [x[\"prompt\"] for x in inputs]\n",
    "prompts_text = [maybe_apply_chat_template(example, tokenizer)[\"prompt\"] for example in inputs]\n",
    "prompt_inputs = tokenizer(\n",
    "        prompts_text, return_tensors=\"pt\", padding=True, padding_side=\"left\", add_special_tokens=False\n",
    "    )\n",
    "prompt_inputs.to(device)\n",
    "prompt_ids, prompt_mask = prompt_inputs[\"input_ids\"], prompt_inputs[\"attention_mask\"]\n",
    "print(prompt_ids.shape, prompt_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\\nUser: How many units long is a segment whose endpoints are $(-4,1)$ and $(1,13)$?\\nAssistant: <think>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate(prompts_text, sampling_params=sampling_params, use_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.trainer.utils import generate_model_card, get_comet_experiment_url, pad, selective_log_softmax\n",
    "\n",
    "device =  \"cuda:0\"\n",
    "completion_ids = [out.token_ids for completions in outputs for out in completions.outputs]\n",
    "\n",
    "# Pad the completions, and concatenate them with the prompts\n",
    "completion_ids = [torch.tensor(ids, device=device) for ids in completion_ids]\n",
    "completion_ids = pad(completion_ids, padding_value=tokenizer.pad_token_id)\n",
    "prompt_completion_ids = torch.cat([prompt_ids, completion_ids], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 927]) torch.Size([16, 1155])\n"
     ]
    }
   ],
   "source": [
    "print(completion_ids.shape, prompt_completion_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask everything after the first EOS token\n",
    "is_eos = completion_ids == tokenizer.eos_token_id\n",
    "eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device)\n",
    "eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]\n",
    "sequence_indices = torch.arange(is_eos.size(1), device=device).expand(is_eos.size(0), -1)\n",
    "completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1155]) 927\n"
     ]
    }
   ],
   "source": [
    "# Concatenate prompt_mask with completion_mask for logit computation\n",
    "attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)  # (B*G, P+C)\n",
    "logits_to_keep = completion_ids.size(1)  # we only need to compute the logits for the completion tokens\n",
    "print(attention_mask.shape, logits_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.inference_mode():\n",
    "#     logits = model(input_ids=prompt_completion_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1).logits\n",
    "# print(logits.shape)\n",
    "# logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred\n",
    "# print(logits.shape)\n",
    "# input_ids = prompt_completion_ids[:, -logits_to_keep:]\n",
    "# print(input_ids.shape)\n",
    "# logits = logits[:, -logits_to_keep:]\n",
    "# print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated completions\n",
    "completions_text = tokenizer.batch_decode(completion_ids, skip_special_tokens=True)\n",
    "prompt_completion_text = tokenizer.batch_decode(prompt_completion_ids,  skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_conversational(inputs[0]):\n",
    "    completions = []\n",
    "    for prompt, completion in zip(prompts, completions_text):\n",
    "        bootstrap = prompt.pop()[\"content\"] if prompt[-1][\"role\"] == \"assistant\" else \"\"\n",
    "        completions.append([{\"role\": \"assistant\", \"content\": bootstrap + completion}])\n",
    "else:\n",
    "    completions = completions_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyalign.reward.openr1_reward import  (\n",
    "    accuracy_reward,\n",
    "    format_reward,\n",
    "    get_cosine_scaled_reward,\n",
    "    get_repetition_penalty_reward,\n",
    "    len_reward,\n",
    "    reasoning_steps_reward,\n",
    "    tag_count_reward, \n",
    "    understand_reward\n",
    ")\n",
    "rewards = understand_reward(prompts, completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rewards[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content, sol in zip(prompts, completions):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' We have two points, A(-4,1) and B(1,13), and we want to find the length of the segment connecting these two points. To do this, we can use the distance formula: distance = √[(x2-x1)² + (y2-y1)²]  <answer> According to the distance formula, if we substitute A(-4,1) and B(1,13) into the formula, we get distance = √[(1 - (-4))² + (13 - 1)²]  \\ndistance = √[5² + 12²]  \\ndistance = √[25 + 144]  \\ndistance = √169  \\ndistance = 13 <answer> </answer> </answer>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\\nUser: How many units long is a segment whose endpoints are $(-4,1)$ and $(1,13)$?\\nAssistant: <think>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"</think> <answer>\" in sol and \"</answer>\" in sol:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyalign.reward.openr1_reward import  (\n",
    "    accuracy_reward,\n",
    "    format_reward,\n",
    "    get_cosine_scaled_reward,\n",
    "    get_repetition_penalty_reward,\n",
    "    len_reward,\n",
    "    reasoning_steps_reward,\n",
    ")\n",
    "\n",
    "cosine_min_value_wrong = 0\n",
    "cosine_max_value_wrong = 0.5\n",
    "cosine_min_value_correct = -0.5\n",
    "cosine_max_value_correct = 1.0\n",
    "cosine_max_len = 1000\n",
    "repetition_n_grams = 3\n",
    "repetition_max_penalty = -1\n",
    "REWARD_FUNCS_REGISTRY = {\n",
    "    \"accuracy\": accuracy_reward,\n",
    "    \"format\": format_reward,\n",
    "    \"reasoning_steps\": reasoning_steps_reward,\n",
    "    \"cosine\": get_cosine_scaled_reward(\n",
    "        min_value_wrong=cosine_min_value_wrong,\n",
    "        max_value_wrong=cosine_max_value_wrong,\n",
    "        min_value_correct=cosine_min_value_correct,\n",
    "        max_value_correct=cosine_max_value_correct,\n",
    "        max_len=cosine_max_len,\n",
    "    ),\n",
    "    \"repetition_penalty\": get_repetition_penalty_reward(\n",
    "        ngram_size=repetition_n_grams,\n",
    "        max_penalty=repetition_max_penalty,\n",
    "    ),\n",
    "    \"length\": len_reward,\n",
    "}\n",
    "reward_funcs = [REWARD_FUNCS_REGISTRY[func] for func in [\"accuracy\", \"format\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We need for the value of the function at x=-2, x=2, and x=2 to be the same. So, we'll put the value of x=-2 in the first equation, x=2 in the middle equation, and x=2 in the last equation. Let's do that. We get: a(-2)+3 = -2-5 = -7. And a+3 = 0. So, a = -3. The last value we can find out is b, for when x=2, we use the expression from the second equation. So 2(-3)-b = 0, b = -6. Therefore, a+b = -3+(-6) = -9. </think>\n",
      "<answer>-9</answer>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m keys = [key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m inputs[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m      8\u001b[39m reward_kwargs = {key: [example[key] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m inputs] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys}\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m output_reward_func = \u001b[43mreward_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mreward_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data4/gwy/new-easy-align/easyalign/reward/openr1_reward.py:14\u001b[39m, in \u001b[36maccuracy_reward\u001b[39m\u001b[34m(completions, solution, **kwargs)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(completions[\u001b[32m0\u001b[39m])\n\u001b[32m     13\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Reward function that checks if the completion is the same as the ground truth.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m contents = \u001b[43m[\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     15\u001b[39m rewards = []\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m content, sol \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(contents, solution):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data4/gwy/new-easy-align/easyalign/reward/openr1_reward.py:14\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(completions[\u001b[32m0\u001b[39m])\n\u001b[32m     13\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Reward function that checks if the completion is the same as the ground truth.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m contents = [\u001b[43mcompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m completion \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[32m     15\u001b[39m rewards = []\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m content, sol \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(contents, solution):\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "reward_processing_classes = [None] * len(reward_funcs)\n",
    "rewards_per_func = torch.zeros(len(prompts), len(reward_funcs), device=device)\n",
    "for i, (reward_func, reward_processing_class) in enumerate(\n",
    "    zip(reward_funcs, reward_processing_classes)\n",
    "):\n",
    "    # Repeat all input columns (but \"prompt\" and \"completion\") to match the number of generations\n",
    "    keys = [key for key in inputs[0] if key not in [\"prompt\", \"completion\"]]\n",
    "    reward_kwargs = {key: [example[key] for example in inputs] for key in keys}\n",
    "    output_reward_func = reward_func(prompts=prompts, completions=completions, **reward_kwargs)\n",
    "    rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_per_func"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
